{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22089b92-4ada-4580-9eac-6b9e2cd89812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\Users\\\\SkJain\\\\Downloads\\\\Compressed\\\\winutils-master\\\\hadoop-3.2.2\"\n",
    "os.environ[\"HIVE_HOME\"] = \"C:\\\\Users\\\\SkJain\\\\Documents\\\\BigDataStackWorkspace\\\\SparkLearn\\\\apache-hive-3.1.3-bin\"\n",
    "os.environ[\"HIVE_LIB\"] = \"C:\\\\Users\\\\SkJain\\\\Documents\\\\BigDataStackWorkspace\\\\SparkLearn\\\\apache-hive-3.1.3-bin\\\\lib\"\n",
    "os.environ[\"HIVE_BIN\"] =  \"C:\\\\Users\\\\SkJain\\\\Documents\\\\BigDataStackWorkspace\\\\SparkLearn\\\\apache-hive-3.1.3-bin\\\\bin\"\n",
    "os.environ[\"HADOOP_USER_CLASSPATH_FIRST\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877109e4-5f56-4b43-a851-ac76a522adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\", \"0\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName('SparkSql'). \\\n",
    "    master('local'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d6e02-cd38-4828-a8c7-33be251ae1b3",
   "metadata": {},
   "source": [
    "### Creating table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990d79dc-777e-44f2-8622-5a36b7ec5b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we should do all pre requisites like creating db if it doen't exist, \n",
    "# then switching to the correct db and drop the table which we are going to create in case it already exists\n",
    "# we can use \"SHOW TABLES\" to see all the tables in current database\n",
    "\n",
    "#now to create the table\n",
    "create_order_query = \"\"\" CREATE TABLE ORDERS (\n",
    "    order_id INT,\n",
    "    order_date STRING,\n",
    "    order_cust_id INT,\n",
    "    order_status STRING\n",
    "    ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE\n",
    "\"\"\"\n",
    "# by default also it stores as textfile, so we can skip that line also\n",
    "spark.sql(create_order_query)\n",
    "\n",
    "# to insert data into the table, we can use the insert or load command\n",
    "# insert command is used to manually add one or few records. In real'world scenarios it is less used \n",
    "# since we would be mostly uploading files which can be done using load command\n",
    "\n",
    "# to insert single record using insert command\n",
    "# INSERT INTO orders VALUES (col1_value, col2_value ...)\n",
    "\n",
    "#to insert multiple records we can just write them comma separated\n",
    "# INSERT INTO orders VALUES (col1_value1, col2_value1 ...), (col1_value2, col2_value2 ...)\n",
    "\n",
    "# in spark sql we can't partially provide values to only some of the columns like in traditional rdbs. \n",
    "# But this feature is present in hive\n",
    "\n",
    "# if we provide values in wrong order or anyother scenario where the data type of column and value provided \n",
    "# do not match, it will not throw an error but simply store null in place of that value. If typecasting can\n",
    "# happen then it will store the value, so if we give an int where string is expected, it will be typecasted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3306e-7ac8-41de-aedd-1b0cf0967652",
   "metadata": {},
   "source": [
    "### Data Types in spark sql\n",
    " - hive and spark sql are almost same, so most things will apply to hive as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac14673-8f47-499c-b22b-abf88262f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hive manual: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types\n",
    "\n",
    "# check create table from here, it explain all lines we write in the query: \n",
    "# https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTableCreate/Drop/TruncateTable\n",
    "\n",
    "#more complex query, will create tsv file instead of csv and containg arrays and objects as column values\n",
    "query = \"\"\" CREATE TABLE students(\n",
    "    student_id INT,\n",
    "    student_first_name STRING,\n",
    "    student_last_name STRING,\n",
    "    student_phone_numbers ARRAY<STRING>,\n",
    "    student_address STRUCT<street: STRING, city: STRING, state: STRING, zip: STRING>\n",
    "    ) STORED AS TEXTFILE\n",
    "    ROW FORMAT\n",
    "    DELIMITED FIELDS TERMIANTED BY '\\t'\n",
    "    COLLECTION ITEMS TERMINATED BY ','\"\"\"\n",
    "\n",
    "# for each insert statement of new file is created\n",
    "# in the file when we check array values and struct object values will be separated by commas as we mentioned in the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2aa87-2705-4075-9153-eb70bb3132ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can add comment for each column while creating table\n",
    "# these comments will be visible when we use the command \"DESCRIBE tablename\"\n",
    "# we can add it using COMMNET keuword\n",
    "# it can be at both table level as well as column level\n",
    "# to see table level comment w can use \"DESCRIBE FORMATTED tablename\"\n",
    "\n",
    "#eg.\n",
    "create_order_query = \"\"\" CREATE TABLE ORDERS (\n",
    "    order_id INT COMMENT 'Unique order id',\n",
    "    order_date STRING COMMENT 'date on which order was placed',\n",
    "    order_cust_id INT COMMENT 'customer who placed this order',\n",
    "    order_status STRING COMMENT 'current status of the order'\n",
    "    ) COMMENT 'Table to store details of orders' \n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4a0a9-b2ca-406f-85e6-eb427831fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data into tables\n",
    "#the file format we specify in create table query is about how the data will be saved in hive\n",
    "#but it is preferred to load data from an input file in same format, otherwise it won't throw errors but \n",
    "#it will start storing incorrect results\n",
    "\n",
    "#to load data from local file system (LOCAL)\n",
    "load_query = \"LOAD DATA LOCAL INPATH 'filepathOrFolderPath' INTO TABLE ORDERS\"\n",
    "\n",
    "#to load data from HDFS file system (without LOCAL keyword)\n",
    "#user should have write permission on the source\n",
    "#data will be MOVED to spark metastore (deleted from source)\n",
    "load_query = \"LOAD DATA INPATH 'filepathOrFolderPath' INTO TABLE ORDERS\"\n",
    "\n",
    "# append data while loading to an existing table or overwrite all the current data\n",
    "# the query used till now appends the data, \n",
    "load_query_append = \"LOAD DATA INPATH 'filepathOrFolderPath' INTO TABLE ORDERS\"\n",
    "#if the name of the file from which we are loading the data is already present it, it will append _copy_1, _copy_2\n",
    "# and so on to the filename\n",
    "\n",
    "#to overwrite the table we need specify OVERWRITE keyword before INTO\n",
    "load_query_overwrite = \"LOAD DATA INPATH 'filepathOrFolderPath' OVERWRITE INTO TABLE ORDERS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dfef6b-fb10-435c-bfbd-bb616ddf7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External tables in hive\n",
    "# we need to add EXTERNAL keywword after CREATE  while creating a new table\n",
    "# we also need to specify LOCATION at the end which will contain the path of folder in hdfs\n",
    "#It will not be stored in spark metastore but at the mentioned location\n",
    "create_order_query = \"\"\" CREATE EXTERNAL TABLE ORDERS (\n",
    "    order_id INT,\n",
    "    order_date STRING,\n",
    "    order_cust_id INT,\n",
    "    order_status STRING\n",
    "    ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE\n",
    "    LOCATION 'FOLDERpATH'\n",
    "\"\"\"\n",
    "\n",
    "# When we drop a managed table. it deleted all metadata from spark metastore \n",
    "# as well as the actual data from hdfs for this table\n",
    "#For external tables, only metadata from spark metastore is deleted, but the actual data is preserved in that path\n",
    "#Typically extrernal tables are used when same data is processed by multiple frameworks like pig, spark etc.\n",
    "# we cannnot run TRUNCATE command agaonst external tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f96b1e-a94d-4d3c-a6b5-4953adf43f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File formats in hive\n",
    "# file_format:\n",
    "#   : SEQUENCEFILE (outdated)\n",
    "#   | TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)\n",
    "#   | RCFILE      -- (Note: Available in Hive 0.6.0 and later)\n",
    "#   | ORC         -- (Note: Available in Hive 0.11.0 and later)\n",
    "#   | PARQUET     -- (Note: Available in Hive 0.13.0 and later)\n",
    "#   | AVRO        -- (Note: Available in Hive 0.14.0 and later)\n",
    "#   | JSONFILE    -- (Note: Available in Hive 4.0.0 and later)\n",
    "#   | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname\n",
    "\n",
    "# ORC, parquet and avro are most popular among all file formals\n",
    "\n",
    "query_avro = \"\"\" CREATE TABLE students(\n",
    "    student_id INT,\n",
    "    student_first_name STRING,\n",
    "    student_last_name STRING,\n",
    "    student_phone_numbers ARRAY<STRING>,\n",
    "    student_address STRUCT<street: STRING, city: STRING, state: STRING, zip: STRING>\n",
    "    ) STORED AS AVRO\"\"\"\n",
    "\n",
    "query_parquet = \"\"\" CREATE TABLE students(\n",
    "    student_id INT,\n",
    "    student_first_name STRING,\n",
    "    student_last_name STRING,\n",
    "    student_phone_numbers ARRAY<STRING>,\n",
    "    student_address STRUCT<street: STRING, city: STRING, state: STRING, zip: STRING>\n",
    "    ) STORED AS PARQUET\"\"\"\n",
    "\n",
    "#PARQUET FILES BY DEFULT use snappy as compression. so files extension will be .snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50093ca7-23cc-4c66-954c-9651f2989173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to drop table\n",
    "# DROP TABLE tablename\n",
    "# DROP TABLE IF EXISTS tablename\n",
    "\n",
    "#DROP DATABASE IF EXISTS dbname\n",
    "#DROP DATABASE IF EXISTS dbname CASCADE\n",
    "\n",
    "#truncate TABLE (data is removed but structure of table is preserved)\n",
    "#only works for managed tables\n",
    "TRUNCATE TABLE tablename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
