{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e608a5-4d8a-4022-a6bc-fbfcac174d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\Users\\\\SkJain\\\\Downloads\\\\Compressed\\\\winutils-master\\\\hadoop-3.2.2\"\n",
    "os.environ[\"HIVE_HOME\"] = \"C:\\\\Users\\\\SkJain\\\\Documents\\\\BigDataStackWorkspace\\\\SparkLearn\\\\apache-hive-3.1.3-bin\"\n",
    "os.environ[\"HIVE_LIB\"] = \"C:\\\\Users\\\\SkJain\\\\Documents\\\\BigDataStackWorkspace\\\\SparkLearn\\\\apache-hive-3.1.3-bin\\\\lib\"\n",
    "os.environ[\"HIVE_BIN\"] =  \"C:\\\\Users\\\\SkJain\\\\Documents\\\\BigDataStackWorkspace\\\\SparkLearn\\\\apache-hive-3.1.3-bin\\\\bin\"\n",
    "os.environ[\"HADOOP_USER_CLASSPATH_FIRST\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27174aa5-2d4a-4dc7-ac19-878f6613ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\", \"0\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName('SparkSql'). \\\n",
    "    master('local'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "733cfaa4-ac42-48d4-b53b-4cafd195b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession. \\\n",
    "#     builder. \\\n",
    "#     config(\"spark.ui.port\", \"0\"). \\     \n",
    "#     enableHiveSupport(). \\\n",
    "#     appName('SparkSql'). \\\n",
    "#     master('local'). \\\n",
    "#     getOrCreate()\n",
    "# # config(\"spark.sql.warehouse.dir\",\"./spark-warehouse\"). \\config(\"spark.sql.catalogImplementation\",\"hive\"). \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d16f3-f5a3-4472-b8e6-ec365c060940",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "**Get Daily product revenue**\n",
    "- This is the complete use case and will mostly use all major types of spark sql operations mentioned in spark notes file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ebee9b-3026-41e1-96c4-f6cf27e207be",
   "metadata": {},
   "source": [
    "## Prepare Tables\n",
    "- tables needed:\n",
    "    - orders\n",
    "    - order_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87927746-e4ef-4897-aed5-070be72fd718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_database()\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9abdba5b-78bd-45d5-aa74-d1906d87b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS siddhantdb\")\n",
    "spark.sql(\"SHOW databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a836153-7121-4397-8e33-db05b6a7cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE siddhantdb\")\n",
    "spark.sql(\"SELECT current_database()\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a41fcc6-7908-48bf-8027-84138089ef2f",
   "metadata": {},
   "source": [
    "### Creating Orders Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3e0b8-100e-412f-a56f-08f7001b93bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS Orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1b794-f774-4912-ad5f-a8632784cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_order_query = \"\"\" CREATE TABLE ORDERS (\n",
    "    order_id INT,\n",
    "    order_date STRING,\n",
    "    order_cust_id INT,\n",
    "    order_status STRING\n",
    "    ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_order_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95120912-716b-4548-ac2d-f33178a5a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data in the above created table\n",
    "ordersFilePath = 'datasets/orders/*'\n",
    "load_data_query = f\"LOAD DATA LOCAL INPATH '{ordersFilePath}' INTO TABLE orders\"\n",
    "spark.sql(load_data_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3508c7b-4c7a-4236-a5ba-0a07d16a146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM ORDERS LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8c5fa-62f9-4324-87e5-3a6d4dd78ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(1) FROM ORDERS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d74b1-0cef-4806-adc5-32406bba42cc",
   "metadata": {},
   "source": [
    "### Creating Orders Item Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6edff9c3-e7ec-4727-a2b9-a7ca4ea3a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f6ac1b4-4539-4756-b867-4e19c59e69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_order_item_query = \"\"\" CREATE TABLE order_items (\n",
    "    order_item_id INT,\n",
    "    order_item_order_id INT,\n",
    "    order_item_prod_id INT,\n",
    "    order_item_quantity INT,\n",
    "    order_item_subtotal FLOAT,\n",
    "    order_item_prod_price FLOAT\n",
    "    ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_order_item_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f8752-6e0a-4cd3-b29b-c5d49e46005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItemsFilePath = 'datasets/order_items/*'\n",
    "load_data_query = f\"LOAD DATA LOCAL INPATH '{orderItemsFilePath}' INTO TABLE order_items\"\n",
    "spark.sql(load_data_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506420b-7846-4d85-ab40-da9e91971b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM order_items LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5eb9e4-9e0c-4969-b946-2441d258fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(1) FROM order_items\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e5f00-20a9-43fd-b6b7-b381b3793c06",
   "metadata": {},
   "source": [
    "### Projection\n",
    "- selecting only the required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e274c-fef3-4a3a-9b0b-1b3d4c4b53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to know what all columns are there in the table\n",
    "spark.sql(\"DESCRIBE orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93db56-fe25-427c-a06e-e54c8ce24097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting a subset of columns which we need\n",
    "spark.sql(\"SELECT order_cust_id, order_date, order_status FROM order_items\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0993f6-debf-4551-ae01-5004284508ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a derived column based on our usecase, here we only need month and year part of order date\n",
    "\n",
    "query = \"\"\" SELECT order_cust_id, \n",
    "    date_format(order_date, 'yyyy-MM') as order_month, \n",
    "    order_status FROM order_items \"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a421af6-9f5e-4a86-8414-c7bb1cefa816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only distinct values from a column, or genrally eliminate complete same records\n",
    "spark.sql(\"SELECT DISTINCT order_status FROM order_items\").show()\n",
    "spark.sql(\"SELECT DISTINCT * FROM order_items\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b4b48a-f095-4f9c-a83f-1b952632e540",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "- selecting only the required data from entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725578be-e6ad-446c-b00d-93d2d54f3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get only records where order status has this value\n",
    "spark.sql(\"SELECT * FROM order_items WHERE order_status='COMPLETE'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff85f2-caff-45fd-8a25-a2721df4bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for checking in multiple order status\n",
    "spark.sql(\"SELECT * FROM order_items WHERE order_status IN ('COMPLETE', 'CLOSED')\").show()\n",
    "#or (below approah is used when conditions are on different column. For this usecase we should prefer 'IN'\n",
    "spark.sql(\"SELECT * FROM order_items WHERE order_status = 'COMPLETE' OR order_status = 'CLOSED')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e28e48-235e-4c34-98a5-6a0540e8f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple conditions and pattern matching\n",
    "#getting all records with above mentioned order statuses and placed in Jan-2014\n",
    "query = \"\"\" SELECT * FROM order_items WHERE order_status IN ('COMPLETE', 'CLOSED')\n",
    "    AND order_date LIKE '2014-01-%'\"\"\"\n",
    "spark.sql(\"\").show()\n",
    "\n",
    "#derived column in conditions\n",
    "query = \"\"\" SELECT * FROM order_items WHERE order_status IN ('COMPLETE', 'CLOSED')\n",
    "    AND date_format(order_date, 'yyyy-MM')='2014-01'\"\"\"\n",
    "spark.sql(\"\").show()\n",
    "\n",
    "# use is null and is not null to check for null values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
