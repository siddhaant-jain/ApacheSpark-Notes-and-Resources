{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b28414-a2c8-41d4-b4fd-d95295c73e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\Users\\\\SkJain\\\\Downloads\\\\Compressed\\\\winutils-master\\\\hadoop-3.2.2\"\n",
    "sys.path.append('C:\\\\Users\\\\SkJain\\\\Downloads\\\\Compressed\\\\winutils-master\\\\hadoop-3.2.2\\\\bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14e5075-2ed4-4797-9927-7aa63b48c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\", \"0\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName('SparkSql'). \\\n",
    "    master('local'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7a7cb-bb5b-4a92-a9f3-f6da2229510b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## List of function and see how to use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c9ab8c7-3550-44df-b837-5a791c638675",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW FUNCTIONS\").show(300, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0fa1565-8e66-4abe-b5f7-5061b067a257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Function: substr                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.Substring                                                                                                                                                                                                                                                                                                                             |\n",
      "|Usage: \n",
      "    substr(str, pos[, len]) - Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.\n",
      "\n",
      "    substr(str FROM pos[ FOR len]]) - Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.\n",
      "  |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION substr\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3914f6a-0520-43a2-8a03-56d405b55f89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Validating functions\n",
    "\n",
    "- SparkSql follows mysql style. \n",
    "- We can simply write queries like SELECT current_date; or SELECT SUBSTR('Hello World',1,5)\n",
    "- if we want to do it oracle style we can create a dummy table named 'dual' with one column named dummy and insert one value into it 'X'\n",
    "- then we can write our queries like: \"SELECT SUBSTR('Hello World',1,5) result FROM dual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee0d43a7-5e15-4a55-8e9e-8840987e689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2022-04-14|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794db4a5-c1e6-481a-a025-d6563b7444ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|result|\n",
      "+------+\n",
      "| Hello|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT SUBSTR('Hello World',1,5) result\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a929118-d804-4ecb-9e4d-238c94776470",
   "metadata": {
    "tags": []
   },
   "source": [
    "## String Manipulation functions\n",
    "- case conversion: lower, upper, initcap\n",
    "- size of column value: length\n",
    "- extracting data: substr, split (substr and substring both do the same thing)\n",
    "- triminng and padding: trim, rtrim, ltrim\n",
    "    - removes extra white spaces from both sides, right side and left side respectively\n",
    "- padding: rpad, lpad\n",
    "    - selct lpad(column, desired_length, character_to_pad on left)\n",
    "    - if column is less than of desired length, then character_to_pad is added until desired length is achieved\n",
    "    - if length of column is same as desired length then nothing happens\n",
    "    - if length is more than lpad will trim characters from right\n",
    "- reverse string: reverse\n",
    "- concatenate multiple strings: concat and concat_ws \n",
    "    - ws stands for 'with separator'\n",
    "    - used when we want to concatenate multiple string but with same separator in between (eg. space for name or hyphen for date)\n",
    "    - first value in concat_ws is the separator followed by actual strings\n",
    "- explode (to convert one record into multiple records) : eg SELECT explode(split('2013-07-25', '-'))\n",
    "    - above query will create 3 rows with values 2013, 07,25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7be8d-f0d3-4412-81b4-8061cfedf235",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Date Manipulation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c7975-e1b3-4e82-a0fd-871966f6c8c8",
   "metadata": {},
   "source": [
    "### Getting current date or current timestamp\n",
    "    - both these functions are not listed in SHOW FUNCTIONS, but DESCRIBE still works\n",
    "    - dates are nothing but special string, so all string related functions can be used on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "853ec15f-f626-4f7e-99c3-c78fb9430c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2022-04-14|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_date\").show() #default format: yyyy-MM-dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f71a6347-e34b-4c55-9a47-176e85dd9c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|current_timestamp()    |\n",
      "+-----------------------+\n",
      "|2022-04-14 20:43:32.863|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp\").show(truncate=False) #default format: yyyy-MM-dd HH:mm:ss.SSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac53e87-5af1-41c2-b881-491e9877630d",
   "metadata": {},
   "source": [
    "### Date Arithmetic\n",
    "- date_add\n",
    "- date_sub\n",
    "- datediff\n",
    "- add_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65c396a8-4064-49c5-8e9f-9465b673062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|date_add(current_date(), 10)|\n",
      "+----------------------------+\n",
      "|2022-04-24                  |\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT date_add(current_date, 10)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4394e785-86f4-4341-aa38-0c9851225e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|date_add(current_date(), 365)|\n",
      "+-----------------------------+\n",
      "|2023-04-14                   |\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT date_add(current_date, 365)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b2bb02a-853d-44fb-986e-42dcd5ee296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|date_add(current_date(), -365)|\n",
      "+------------------------------+\n",
      "|2021-04-14                    |\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT date_add(current_date, -365)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb7d9602-5ba3-4170-9667-303269827524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|date_sub(current_date(), 365)|\n",
      "+-----------------------------+\n",
      "|2021-04-14                   |\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT date_sub(current_date, 365)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b43a50c-7f87-4dd1-b78b-179bf7fa5f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|datediff(CAST(2022-04-14 AS DATE), CAST(2022-01-01 AS DATE))|\n",
      "+------------------------------------------------------------+\n",
      "|103                                                         |\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT datediff('2022-04-14', '2022-01-01')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae37ac7c-dbde-4f05-a271-93c44e7d57d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|add_months(CAST(2022-04-20 AS DATE), 9)|\n",
      "+---------------------------------------+\n",
      "|2023-01-20                             |\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT add_months('2022-04-20', 9)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aae1e340-0791-4bbb-94b8-b92c404112e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|add_months(CAST(2022-01-31 AS DATE), 1)|\n",
      "+---------------------------------------+\n",
      "|2022-02-28                             |\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if we do add month on a border date, it will simply give last date of next month\n",
    "spark.sql(\"SELECT add_months('2022-01-31', 1)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951ce0a-c495-47eb-ad1b-ce937d08e01f",
   "metadata": {},
   "source": [
    "### Begining date or time\n",
    "- trunc()\n",
    "    - works on both dates and timestamps\n",
    "    - can be only used to get begining month or year \n",
    "    - use MM for month and YY for year\n",
    "    - anything except MM and YY will return null\n",
    "- date_trunc()\n",
    "    - get begining time up tp seconds\n",
    "    - takes format first unlike trunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39fdf22e-0695-4dde-bb1e-d6b892cfb436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|trunc(current_date(), MM)|\n",
      "+-------------------------+\n",
      "|2022-04-01               |\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT trunc(current_date, 'MM')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4299d057-0258-4846-b9e0-c5637d8b4eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|trunc(current_date(), YY)|\n",
      "+-------------------------+\n",
      "|2022-01-01               |\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT trunc(current_date, 'YY')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ec5fdd1-bab2-4bb1-b892-a7e7ec095ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "|Function: date_trunc                                                                                                  |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.TruncTimestamp                                                       |\n",
      "|Usage: \n",
      "    date_trunc(fmt, ts) - Returns timestamp `ts` truncated to the unit specified by the format model `fmt`.\n",
      "  |\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION date_trunc\").show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94d44998-6f3c-4b58-8e37-04908995c85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|date_trunc(HOUR, current_timestamp())|\n",
      "+-------------------------------------+\n",
      "|2022-04-15 11:00:00                  |\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT date_trunc('HOUR',current_timestamp)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e3bb540-f3e7-47e4-adc6-1218aed2bdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|date_trunc(DAY, current_timestamp())|\n",
      "+------------------------------------+\n",
      "|2022-04-15 00:00:00                 |\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT date_trunc('DAY',current_timestamp)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f49ac-9d0d-4766-9286-965c897d1c89",
   "metadata": {},
   "source": [
    "### Extracting info using date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15224a5a-46e3-43e5-b6ce-3e61c5ad8d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                                                   |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Function: date_format                                                                                                           |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.DateFormatClass                                                                |\n",
      "|Usage: date_format(timestamp, fmt) - Converts `timestamp` to a value of string in the format specified by the date format `fmt`.|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION date_format\").show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e604d71b-de3d-427c-9d65-894951ad2760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, yyyy)|\n",
      "+-----------------------+---------------------------------------------------------------+\n",
      "|2022-04-15 11:24:08.091|2022                                                           |\n",
      "+-----------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'yyyy')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c97b756-945d-4f2b-9241-15eead93fba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, yy)|\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "|2022-04-15 11:24:23.171|22                                                           |\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'yy')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a541e7a1-901d-4ceb-87d7-8eb0e02660d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, MM)|\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "|2022-04-15 11:24:40.108|04                                                           |\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'MM')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b47463c6-a972-40b4-99cf-2b648b52f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, dd)|\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "|2022-04-15 11:24:49.087|15                                                           |\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'dd')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e309934-8cd6-4be1-b816-ffadfaaf6dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, MMM)|\n",
      "+-----------------------+--------------------------------------------------------------+\n",
      "|2022-04-15 11:27:52.166|Apr                                                           |\n",
      "+-----------------------+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'MMM')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "578b9de1-4956-4a0b-b803-e31685ac6639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, MMMM)|\n",
      "+-----------------------+---------------------------------------------------------------+\n",
      "|2022-04-15 11:28:05.646|April                                                          |\n",
      "+-----------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'MMMM')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37bce5f2-04cd-46f1-9f51-d4e93e910c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, EE)|\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "|2022-04-15 11:28:14.311|Fri                                                          |\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'EE')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad1731e7-f977-48fa-b4fa-a06e774c08a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, EEEE)|\n",
      "+-----------------------+---------------------------------------------------------------+\n",
      "|2022-04-15 11:28:29.638|Friday                                                         |\n",
      "+-----------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'EEEE')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbd5bad3-63ef-432a-84cf-be50f9f985c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------+\n",
      "|current_timestamp()    |24HrFormat|\n",
      "+-----------------------+----------+\n",
      "|2022-04-15 11:29:16.483|11        |\n",
      "+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'HH') as 24HrFormat\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed617620-f2d5-4e6b-9abe-35f11769a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------+\n",
      "|current_timestamp()    |12HrFormat|\n",
      "+-----------------------+----------+\n",
      "|2022-04-15 11:29:16.785|11        |\n",
      "+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'hh') as 12HrFormat\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02fb47e4-bed6-437e-9e1a-7e8ae0b38250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, mm)|\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "|2022-04-15 11:29:30.299|29                                                           |\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'mm')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8842c4cc-6e24-4285-be63-14627e9cb949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, ss)|\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "|2022-04-15 11:29:37.915|37                                                           |\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'ss')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83999806-7815-4f30-94e9-43a9b53abda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, SS)|\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "|2022-04-15 11:29:51.431|43                                                           |\n",
      "+-----------------------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, date_format(current_timestamp, 'SS')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72faaf2b-cb0b-41ac-9edb-f6f999b81e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------------------------------------------------------------+\n",
      "|current_timestamp()    |date_format(current_timestamp() AS `current_timestamp()`, EEEE dd/MM/yy)|\n",
      "+-----------------------+------------------------------------------------------------------------+\n",
      "|2022-04-15 11:33:03.374|Friday 15/04/22                                                         |\n",
      "+-----------------------+------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert to a diffrent style of writing date\n",
    "spark.sql(\"\"\"SELECT current_timestamp, \n",
    "          date_format(current_timestamp ,'EEEE dd/MM/yy')\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f9053-fb27-449f-87ae-ed135a25a550",
   "metadata": {},
   "source": [
    "### Calendar functions\n",
    "- get values such as day, dayOfMonth, month, weekOfYear, year etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be731c08-0ba5-483b-8a6f-cb3107520cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|function_desc                                                     |\n",
      "+------------------------------------------------------------------+\n",
      "|Function: day                                                     |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.DayOfMonth       |\n",
      "|Usage: day(date) - Returns the day of month of the date/timestamp.|\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION day\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9929db35-ab54-4d7c-9152-c76ba590f4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------------------------------------------------+\n",
      "|current_timestamp()    |day(CAST(current_timestamp() AS `current_timestamp()` AS DATE))|\n",
      "+-----------------------+---------------------------------------------------------------+\n",
      "|2022-04-15 11:35:39.669|15                                                             |\n",
      "+-----------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, day(current_timestamp)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57542ff9-92cc-49e1-abf2-362778fba678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------------------------------------------------+\n",
      "|current_timestamp()    |year(CAST(current_timestamp() AS `current_timestamp()` AS DATE))|\n",
      "+-----------------------+----------------------------------------------------------------+\n",
      "|2022-04-15 11:36:08.016|2022                                                            |\n",
      "+-----------------------+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, year(current_timestamp)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f80370c9-1ad2-4908-b123-f535a3d5deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------------------------------------------------+\n",
      "|current_timestamp()    |month(CAST(current_timestamp() AS `current_timestamp()` AS DATE))|\n",
      "+-----------------------+-----------------------------------------------------------------+\n",
      "|2022-04-15 11:36:13.323|4                                                                |\n",
      "+-----------------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, month(current_timestamp)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10304e9e-a7ed-4fe4-ae6a-f4162db82545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------------------------------------------------------+\n",
      "|current_timestamp()    |weekofyear(CAST(current_timestamp() AS `current_timestamp()` AS DATE))|\n",
      "+-----------------------+----------------------------------------------------------------------+\n",
      "|2022-04-15 11:36:21.758|15                                                                    |\n",
      "+-----------------------+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, weekofyear(current_timestamp)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0cc4b69d-c17e-4356-bfd1-76012ed75038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------------------------------------------------------+\n",
      "|current_timestamp()    |dayofmonth(CAST(current_timestamp() AS `current_timestamp()` AS DATE))|\n",
      "+-----------------------+----------------------------------------------------------------------+\n",
      "|2022-04-15 11:36:52.437|15                                                                    |\n",
      "+-----------------------+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, dayofmonth(current_timestamp)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45d288-e70c-4133-bf87-678bb76ff702",
   "metadata": {},
   "source": [
    "### Dealing with unix timestamp\n",
    "- unix timestamp is an intger value which started somewhere in 1970 and is incremented every second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e221741e-75a3-41f1-96af-d2cc76f46d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------------------------------------------------------------------------+\n",
      "|current_timestamp()    |to_unix_timestamp(current_timestamp() AS `current_timestamp()`, yyyy-MM-dd HH:mm:ss)|\n",
      "+-----------------------+------------------------------------------------------------------------------------+\n",
      "|2022-04-15 11:39:40.631|1650002980                                                                          |\n",
      "+-----------------------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, to_unix_timestamp(current_timestamp)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "768d0a57-17ee-44c6-9ae0-3209c847b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------------------------------------------------+\n",
      "|current_timestamp()    |from_unixtime(CAST(1650004000 AS BIGINT), yyyy-MM-dd HH:mm:ss)|\n",
      "+-----------------------+--------------------------------------------------------------+\n",
      "|2022-04-15 11:40:20.784|2022-04-15 11:56:40                                           |\n",
      "+-----------------------+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, from_unixtime(1650004000)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "652029a1-9c3c-4212-8eb6-441ac20e5873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------------------------------------+\n",
      "|current_timestamp()   |from_unixtime(CAST(1650004000 AS BIGINT), yyyy-MM)|\n",
      "+----------------------+--------------------------------------------------+\n",
      "|2022-04-15 11:42:31.98|2022-04                                           |\n",
      "+----------------------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_timestamp, from_unixtime(1650004000, 'yyyy-MM')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926dba86-a6d4-4bdb-ba75-4e1e3c67c33e",
   "metadata": {},
   "source": [
    "## Numeric Functions\n",
    "\n",
    "- abs\n",
    "- sum, avg\n",
    "- round\n",
    "    - we can round to nearest integer (based on greater or less than .5), if we only provide col_name/ data as parameter\n",
    "    - we can round to certain precision if we give a second parameter (eg. round(12.587,1) will give 12.6)\n",
    "- ceil, floor\n",
    "- greatest\n",
    "- min, max\n",
    "- rand (to generate a random number)\n",
    "    - eg. select rand() as random_number\n",
    "    - by default it gives between 0 and 1\n",
    "    - if we want to get an integer between 1 and 100 we can write: select cast(round(rand()*100), int) \n",
    "    - if we want only 0 or 1: select cast(round(rand()*1), int)\n",
    "- pow, sqrt\n",
    "- cumedist, stddev, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50190df-07a7-453d-a071-13c4d9f2e7d0",
   "metadata": {},
   "source": [
    "## Data type conversion\n",
    "- if cannot be casted then it return null (will not throw any error)\n",
    "- CAST(col_name as INT)\n",
    "- CAST(col_name as FLOAT)\n",
    "- CAST(col_name as TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801606ab-1524-4bcd-b3de-ea68a9959950",
   "metadata": {},
   "source": [
    "## Dealing With Nulls\n",
    "- any operation on null will return null\n",
    "- we can use nvl or coalesce\n",
    "- nvl takes 2 argument column/data and the value to replace with if null (for eg. if it's int we can replace with 0)\n",
    "- coalesce takes multiple values and considers first non null value\n",
    "- nvl2 can be used to perform one action if value is not null and another if it is. It will take 3 args: data, notnull value/formula, null case value/formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db0131-049c-4aba-9b79-1de14d6835b5",
   "metadata": {},
   "source": [
    "## Case When Else Statement\n",
    "syntax:  \n",
    "``` \n",
    "CASE  \n",
    "    WHEN condition1 THEN value1  \n",
    "    WHEN condition2 THEN value2  \n",
    "    WHEN conditionN THEN valueN  \n",
    "    ELSE defaultValue  \n",
    "END AS column_name\n",
    "```  \n",
    "     \n",
    "- if else not given after cases then all values which do not match the condition will be set to null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35a7700-c5b6-44d0-98ea-4d7eee454dd3",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b9a07b2b-9236-41f5-9722-6fd3d3df80dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|default           |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_database()\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec4ff08d-fd15-4f87-8564-ba099f7ea2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|namespace |\n",
      "+----------+\n",
      "|default   |\n",
      "|nysedb    |\n",
      "|siddhantdb|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4955563e-9172-4f8a-b0e7-9a256e942826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE nysedb\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "145d45f9-e0e1-4a01-8c45-6bf7d1fdbe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+\n",
      "|database|tableName    |isTemporary|\n",
      "+--------+-------------+-----------+\n",
      "|nysedb  |nyse_eod     |false      |\n",
      "|nysedb  |nyse_eod_part|false      |\n",
      "+--------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a416fc51-3769-4746-8198-c860cb46e1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_lines_table = \"\"\" \n",
    "CREATE TABLE IF NOT EXISTS LINES (\n",
    " s String\n",
    ")\n",
    "\"\"\"\n",
    "spark.sql(create_lines_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "55aa1b80-7e47-45c0-9d83-ff0889365076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+\n",
      "|database|tableName    |isTemporary|\n",
      "+--------+-------------+-----------+\n",
      "|nysedb  |lines        |false      |\n",
      "|nysedb  |nyse_eod     |false      |\n",
      "|nysedb  |nyse_eod_part|false      |\n",
      "+--------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1bdc2dfd-4204-4df7-b03c-e8b02b32c64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_data_path = './datasets/wordCountOnSparkDocuemntation.txt'\n",
    "load_data_query = f\"LOAD DATA LOCAL INPATH '{word_count_data_path}' INTO TABLE lines\"\n",
    "spark.sql(load_data_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "08f07561-043d-4b28-ab3f-ba2cb1bebd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   s|\n",
      "+--------------------+\n",
      "|      Spark Overview|\n",
      "|Apache Spark is a...|\n",
      "|                    |\n",
      "|         Downloading|\n",
      "|Get Spark from th...|\n",
      "|                    |\n",
      "|If you’d like to ...|\n",
      "|                    |\n",
      "|Spark runs on bot...|\n",
      "|                    |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lines LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e5c6f192-68d1-4170-aa2d-9d62772bf54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      94|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM lines\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a6fab7d6-c7f4-45f7-9ec0-1b9f133f3810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                                                             |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Function: split                                                                                                                           |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringSplit                                                                              |\n",
      "|Usage: split(str, regex, limit) - Splits `str` around occurrences that match `regex` and returns an array with a length of at most `limit`|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE  FUNCTION split\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8eee607f-95f7-4658-a68c-19b4297d9f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[Spark, Overview]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|[Apache, Spark, is, a, unified, analytics, engine, for, large-scale, data, processing., It, provides, high-level, APIs, in, Java,, Scala,, Python, and, R,, and, an, optimized, engine, that, supports, general, execution, graphs., It, also, supports, a, rich, set, of, higher-level, tools, including, Spark, SQL, for, SQL, and, structured, data, processing,, MLlib, for, machine, learning,, GraphX, for, graph, processing,, and, Structured, Streaming, for, incremental, computation, and, stream, processing.]                                                  |\n",
      "|[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[Downloading]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|[Get, Spark, from, the, downloads, page, of, the, project, website., This, documentation, is, for, Spark, version, 3.2.1., Spark, uses, Hadoop’s, client, libraries, for, HDFS, and, YARN., Downloads, are, pre-packaged, for, a, handful, of, popular, Hadoop, versions., Users, can, also, download, a, “Hadoop, free”, binary, and, run, Spark, with, any, Hadoop, version, by, augmenting, Spark’s, classpath., Scala, and, Java, users, can, include, Spark, in, their, projects, using, its, Maven, coordinates, and, Python, users, can, install, Spark, from, PyPI.]|\n",
      "|[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[If, you’d, like, to, build, Spark, from, source,, visit, Building, Spark.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[Spark, runs, on, both, Windows, and, UNIX-like, systems, (e.g., Linux,, Mac, OS),, and, it, should, run, on, any, platform, that, runs, a, supported, version, of, Java., This, should, include, JVMs, on, x86_64, and, ARM64., It’s, easy, to, run, locally, on, one, machine, —, all, you, need, is, to, have, java, installed, on, your, system, PATH,, or, the, JAVA_HOME, environment, variable, pointing, to, a, Java, installation.]                                                                                                                                |\n",
      "|[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT split(s, ' ') as words FROM lines LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "766899ed-18dd-4a42-a086-e47d16868d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|words|\n",
      "+-----+\n",
      "|94   |\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(split(s, ' ')) as words FROM lines\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ceae7446-a56e-4c0e-813a-353d39502a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|words    |\n",
      "+---------+\n",
      "|Spark    |\n",
      "|Overview |\n",
      "|Apache   |\n",
      "|Spark    |\n",
      "|is       |\n",
      "|a        |\n",
      "|unified  |\n",
      "|analytics|\n",
      "|engine   |\n",
      "|for      |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT explode(split(s, ' ')) as words FROM lines LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9a7d60b-178f-4494-94bd-ef6e0e3e88f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|944     |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) from (SELECT explode(split(s, ' ')) as words FROM lines)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e5e44590-1c9f-405d-8ac7-298dc5dcb857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|words  |word_freq|\n",
      "+-------+---------+\n",
      "|Spark  |51       |\n",
      "|and    |26       |\n",
      "|a      |23       |\n",
      "|the    |21       |\n",
      "|       |19       |\n",
      "|for    |16       |\n",
      "|to     |16       |\n",
      "|of     |16       |\n",
      "|in     |14       |\n",
      "|on     |13       |\n",
      "|Python |12       |\n",
      "|run    |11       |\n",
      "|with   |10       |\n",
      "|cluster|9        |\n",
      "|API    |8        |\n",
      "|Scala  |8        |\n",
      "|also   |8        |\n",
      "|data   |7        |\n",
      "|is     |7        |\n",
      "|For    |7        |\n",
      "+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT words, COUNT(*) as word_freq from (SELECT explode(split(s, ' ')) as words FROM lines) group by words order by word_freq desc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "196fe2e1-dd0d-484f-889f-67a5c47fda8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|words        |\n",
      "+-------------+\n",
      "|installation.|\n",
      "|online       |\n",
      "|(Scala,      |\n",
      "|include      |\n",
      "|graphs       |\n",
      "|launch       |\n",
      "|Compatibility|\n",
      "|(core        |\n",
      "|2.12/2.13,   |\n",
      "|(Scaladoc)   |\n",
      "|Functions    |\n",
      "|If           |\n",
      "|—            |\n",
      "|API          |\n",
      "|EC2          |\n",
      "|documentation|\n",
      "|(e.g.        |\n",
      "|It’s         |\n",
      "|specifies    |\n",
      "|introduction |\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT distinct words from (SELECT explode(split(s, ' ')) as words FROM lines)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9d079ac1-91b9-4206-a5ee-ecf152cc722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT words)|\n",
      "+---------------------+\n",
      "|477                  |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(distinct words) from (SELECT explode(split(s, ' ')) as words FROM lines)\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
